{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning on EEG data\n",
    "\n",
    "This tutorial walks through some basic tools for applying ML techniques on EEG data through MNE\n",
    "\n",
    "### 1. Importing and preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "\n",
    "# Folder & files containing the data:\n",
    "data_path = '/Users/athina/Documents/Bremen2018/PracticalPrep/'\n",
    "data_file = '817_1_PDDys_ODDBALL_Clean_curated'\n",
    "\n",
    "filename = data_path + data_file\n",
    "\n",
    "# We read the EEG epochs:\n",
    "epochs = mne.read_epochs(filename + '.fif')\n",
    "\n",
    "epochs = epochs['Standard', 'Novel']\n",
    "\n",
    "epochs.filter(l_freq = 0.1, h_freq = 20)\n",
    "epochs.apply_baseline((None, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main information we wish to retain is (a) the data (b) the true labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and get the data:\n",
    "data = epochs._data\n",
    "\n",
    "# and labels:\n",
    "labels = epochs.events[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reminder exercise: how many trials of each condition do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs.event_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Using sklearn to classify EEG data\n",
    "We can now proceed with some mne.decoding and sklearn functionalities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne.decoding import Vectorizer\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'make_pipeline' will create our analysis pipeline from a set of estimators, sklearn-style.\n",
    "'Vectorizer' comes from MNE and will pull together all epochs x time x channels data to a vector of samples x channels\n",
    "'StandardScaler' normalizes the data by their median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = make_pipeline(Vectorizer(), StandardScaler(),\n",
    "                    svm.SVC(kernel='linear',C=1)\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use scikit-learn function 'train_test_split' to obtain a random split of training and test datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test, labels_train, labels_test = train_test_split(data, labels, test_size=0.4, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise: What are the dimensions of the train / test datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can treat 'clf' as any sklearn estimator, and fit the training data to the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(data_train, labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can then use the classifier to predic the labels of the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, we can directly compute the classification score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(data_train,labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise: What is the classification score on the test dataset? What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(data_test,labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can also compute the classification accuracy based on cross-validation. By default, the selected model will be fitted on 3 different splits of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_CV = make_pipeline(Vectorizer(), StandardScaler(),\n",
    "                    svm.SVC(kernel='linear',C=1)\n",
    "                   )\n",
    "\n",
    "cross_val_score(clf_CV, data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can change the number of CV folds by including an additional parameter, 'cv':\n",
    "\n",
    "Remember that for integer or None inputs, if the estimator is a classifier and the labels are binary or multiclass, StratifiedKFold is used. In all other cases, KFold is used (i.e. the folds are made by preserving the percentage of samples for each class):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(clf_CV, data, labels, cv=5)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can now compute the mean and 95% confidence interval of the classification accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise: What happens to the mean accuracy if you increase the CV folds? If you decrease them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(clf_CV, data, labels, cv=10)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we can evaluate multiple metrics for our models at once, like the time it takes to fit them, the train and test scores with the function 'cross_validate':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "cross_validate(clf, data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will first apply PCA to our data and then classify them. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne.decoding import UnsupervisedSpatialFilter\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = UnsupervisedSpatialFilter(PCA(n_components = 30)) # we create an instance of PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_data = pca.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the explained variance for the retained components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.estimator.explained_variance_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we classify the PCA-transformed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validate(clf, pca_data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Optimizing our classifier\n",
    "\n",
    "In the above paradigm we just used a default implementation of SVM with a linear kernel and a hyperparameter C set to 1. Now we will estimate the parameters of SVM through exhaustive Grid Search.\n",
    "\n",
    "'GridSearchCV' exhaustively tests candidate models from a grid of parameter values specified in the following form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'kernel': ['linear', 'rbf'],'C':[5, 10]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use GridSearchCV directly in 'make_pipeline' to proceed with an optimized model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_opt2 = make_pipeline(Vectorizer(), StandardScaler(), GridSearchCV(svm.SVC(), parameters,cv=5))\n",
    "print(clf_opt2)\n",
    "tmp = clf_opt2.fit(data, labels)\n",
    "print(tmp)\n",
    "cross_val_score(clf_opt2,data,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can assess the optimized parameters through:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = clf_opt2.steps[-1][1]\n",
    "tmp.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier that you trained & optimized above may still be biased. Any ideas why?\n",
    "\n",
    "##### Exercise: how could you structure your dataset to make it unbiased?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test, labels_train, labels_test = train_test_split(data, labels, test_size=0.5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_opt2 = make_pipeline(Vectorizer(), StandardScaler(), GridSearchCV(svm.SVC(), parameters,cv=5))\n",
    "clf_opt2.fit(data_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "clf_opt2.steps[-1]\n",
    "perf = cross_val_score(clf_opt2,data_test,labels_test, cv = 5)\n",
    "np.mean(perf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Making a more sophisticated pipeline\n",
    "\n",
    "In the steps above we were still evaluating a rather simplified version of our data, where we considered EEG data of one condition and the other irrespective of time. Now we will take the temporal dimension into account as well and will evaluate how classification accuracy changes across time.\n",
    "\n",
    "For this we will use 'SlidingEstimator' from MNE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne.decoding import SlidingEstimator, cross_val_multiscore\n",
    "\n",
    "clf = make_pipeline(Vectorizer(), StandardScaler(),\n",
    "                    svm.SVC(kernel='linear',C=1)\n",
    "                   )\n",
    "\n",
    "sl = SlidingEstimator(clf) # we apply the sliding estimator to 'clf'\n",
    "\n",
    "CV_score_time = cross_val_multiscore(sl, data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the dimensions of the classification score now? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_score_time.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot the classification accuracy across time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib tk\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(epochs.times, CV_score_time.T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise: Can you now plot the mean classification accuracy over the CV folds as a function of time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(epochs.times, np.mean(CV_score_time,axis = 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important steps of classifying EEG data is to extract and visualize the features of our classifier. In the following case, we will extract the coefficients of SVM across time and plot them as topographic maps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "topos = np.array([svm.SVC(kernel='linear').fit(time_point.T, labels).coef_ * time_point.std(1)\n",
    "                  for time_point in data.T])[:, 0, :]\n",
    "topo_ev = mne.EvokedArray(topos.T, info=epochs.info, tmin=-.1, nave=len(labels))\n",
    "topo_ev.plot_joint(times=[.22, .3, .375, .45]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise: Can you now plot the classification coefficients on the time-points of maximal classification accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Implementing a more sophisticated pipeline\n",
    "\n",
    "In the same pipeline we can additionally integrate dimensionality reduction techniques. How?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
