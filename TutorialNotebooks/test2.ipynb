{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning on EEG data\n",
    "\n",
    "This tutorial walks through some basic tools for applying ML techniques on EEG data through MNE\n",
    "\n",
    "### 1. Importing and preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 817_1_PDDys_ODDBALL_Clean_curated.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -100.00 ...     500.00 ms\n",
      "        0 CTF compensation matrices available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-ad5cf6c242fb>:7: RuntimeWarning: This filename (817_1_PDDys_ODDBALL_Clean_curated.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  epochs = mne.read_epochs(data_file + '.fif')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189 matching events found\n",
      "No baseline correction applied\n",
      "189 matching events found\n",
      "No baseline correction applied\n",
      "Not setting metadata\n",
      "0 projection items activated\n",
      "Setting up band-pass filter from 0.1 - 20 Hz\n",
      "l_trans_bandwidth chosen to be 0.1 Hz\n",
      "h_trans_bandwidth chosen to be 5.0 Hz\n",
      "Filter length of 16501 samples (33.002 sec) selected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-ad5cf6c242fb>:11: RuntimeWarning: filter_length (16501) is longer than the signal (301), distortion is likely. Reduce filter length or filter a longer signal.\n",
      "  epochs.filter(l_freq = 0.1, h_freq = 20)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying baseline correction (mode: mean)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<EpochsFIF  |   160 events (all good), -0.1 - 0.5 sec, baseline [None, 0], ~22.2 MB, data loaded,\n",
       " 'Novel': 30\n",
       " 'Standard': 130>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mne\n",
    "\n",
    "# Folder & files containing the data:\n",
    "data_file = '817_1_PDDys_ODDBALL_Clean_curated'\n",
    "\n",
    "# We read the EEG epochs:\n",
    "epochs = mne.read_epochs(data_file + '.fif')\n",
    "\n",
    "epochs = epochs['Standard', 'Novel']\n",
    "\n",
    "epochs.filter(l_freq = 0.1, h_freq = 20)\n",
    "epochs.apply_baseline((None, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main information we wish to retain is (a) the data (b) the true labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# and get the data:\n",
    "data = epochs._data\n",
    "\n",
    "n_timepoints = data.shape[2]-1\n",
    "# and labels:\n",
    "labels = epochs.events[:,-1]\n",
    "\n",
    "# extract data along a window:\n",
    "win_s = 5 # window size, ms\n",
    "slide_s = 10 # sliding length, ms\n",
    "\n",
    "# slide and get data in our window of interest:\n",
    "for tp in range(0,n_timepoints, win_s):\n",
    "    data_win = data[:,:,tp:tp+slide_s]\n",
    "    # split to CV and test:\n",
    "    data_CV, data_test, labels_CV, labels_test = train_test_split(data, labels, test_size=0.4, random_state=tp)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Using sklearn to classify EEG data\n",
    "We can now proceed with some mne.decoding and sklearn functionalities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne.decoding import Vectorizer\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'make_pipeline' will create our analysis pipeline from a set of estimators, sklearn-style.\n",
    "'Vectorizer' comes from MNE and will pull together all epochs x time x channels data to a vector of samples x channels\n",
    "'StandardScaler' normalizes the data by their median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = make_pipeline(Vectorizer(), StandardScaler(),\n",
    "                    svm.SVC(kernel='linear',C=1)\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use scikit-learn function 'train_test_split' to obtain a random split of training and test datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train, data_test, labels_train, labels_test = train_test_split(data, labels, test_size=0.4, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise: What are the dimensions of the train / test datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can treat 'clf' as any sklearn estimator, and fit the training data to the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(data_train, labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can then use the classifier to predic the labels of the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, we can directly compute the classification score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(data_train,labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise: What is the classification score on the test dataset? What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can also compute the classification accuracy based on cross-validation. By default, the selected model will be fitted on 3 different splits of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_CV = make_pipeline(Vectorizer(), StandardScaler(),\n",
    "                    svm.SVC(kernel='linear',C=1)\n",
    "                   )\n",
    "\n",
    "cross_val_score(clf_CV, data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can change the number of CV folds by including an additional parameter, 'cv':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(clf_CV, data, labels, cv=5)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can now compute the mean and 95% confidence interval of the classification accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise: What happens to the mean accuracy if you increase the CV folds? If you decrease them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(clf_CV, data, labels, cv=10)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we can evaluate multiple metrics for our models at once, like the time it takes to fit them, the train and test scores with the function 'cross_validate':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "cross_validate(clf, data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will first apply PCA to our data and then classify them. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mne.decoding import UnsupervisedSpatialFilter\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = UnsupervisedSpatialFilter(PCA(n_components = 30)) # we create an instance of PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca_data = pca.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the explained variance for the retained components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.estimator.explained_variance_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we classify the PCA-transformed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validate(clf, pca_data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Optimizing our classifier\n",
    "\n",
    "In the above paradigm we just used a default implementation of SVM with a linear kernel and a hyperparameter C set to 1. Now we will estimate the parameters of SVM through exhaustive Grid Search.\n",
    "\n",
    "'GridSearchCV' exhaustively tests candidate models from a grid of parameter values specified in the following form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'kernel': ['linear', 'rbf'],'C':[1, 10]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use GridSearchCV directly in 'make_pipeline' to proceed with an optimized model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_opt2 = make_pipeline(Vectorizer(), StandardScaler(), GridSearchCV(svm.SVC(), parameters,cv=5))\n",
    "print(clf_opt2)\n",
    "tmp = clf_opt2.fit(data, labels)\n",
    "print(tmp)\n",
    "cross_val_score(clf_opt2,data,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can assess the optimized parameters through:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = clf_opt2.steps[-1][1]\n",
    "tmp.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier that you trained & optimized above may still be biased. Any ideas why?\n",
    "\n",
    "##### Exercise: how could you structure your dataset to make it unbiased?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Making a more sophisticated pipeline\n",
    "\n",
    "In the steps above we were still evaluating a rather simplified version of our data, where we considered EEG data of one condition and the other irrespective of time. Now we will take the temporal dimension into account as well and will evaluate how classification accuracy changes across time.\n",
    "\n",
    "For this we will use 'SlidingEstimator' from MNE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne.decoding import SlidingEstimator, cross_val_multiscore\n",
    "\n",
    "clf = make_pipeline(Vectorizer(), StandardScaler(),\n",
    "                    svm.SVC(kernel='linear',C=1)\n",
    "                   )\n",
    "\n",
    "sl = SlidingEstimator(clf) # we apply the sliding estimator to 'clf'\n",
    "\n",
    "CV_score_time = cross_val_multiscore(sl, data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the dimensions of the classification score now? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_score_time.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot the classification accuracy across time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(epochs.times, CV_score_time.T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise: Can you now plot the mean classification accuracy over the CV folds as a function of time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Implementing a more sophisticated pipeline\n",
    "\n",
    "In the same pipeline we can additionally integrate dimensionality reduction techniques. How?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
